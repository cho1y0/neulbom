"""
ì ì‘í˜• ê°ì • í†µí•© ì—”ì§„ (Adaptive Emotion Fusion)
PDF ê¸°ìˆ ë…¸íŠ¸ ê¸°ë°˜ êµ¬í˜„ + MelissaJ ëª¨ë¸ ëŒ€ì‘

í•µì‹¬ ê°œì„ ì‚¬í•­:
1. Z-score ê¸°ë°˜ Pitch Dynamics ë¶„ì„
2. ë™ì  ì‹ ë¢°ë„ ë¶€ìŠ¤íŒ… (Dynamic Confidence Boosting)
3. 3ê°€ì§€ ìƒí™©ë³„ ê°€ì¤‘ì¹˜ ì ìš©
4. MelissaJ 6ê°ì • ëª¨ë¸ ì§€ì›
5. ì˜ì–´ ë ˆì´ë¸” ê°•ì œ í•œê¸€ ë§¤í•‘ (ìˆ˜ì •!)
"""

import torch
import torch.nn.functional as F
import librosa
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification
from config.models import MODELS


class EmotionEnsemble:
    """
    ê°œì„ ëœ ê°ì • ë¶„ì„ ì—”ì§„ (PDF ê¸°ë°˜)
    - ì ì‘í˜• ê°ì • í†µí•©
    - Pitch Dynamics ê¸°ë°˜ ë³´ì •
    - ê°€ë©´ ìš°ìš¸ì¦ íƒì§€
    - MelissaJ 6ê°ì • ëª¨ë¸ ì§€ì›
    """
    
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"â¤ï¸â€ğŸ©¹ ê°œì„ ëœ ê°ì • ë¶„ì„ ì—”ì§„ ì´ˆê¸°í™” (Device: {self.device})")

        try:
            # Configì—ì„œ ëª¨ë¸ëª… ê°€ì ¸ì˜¤ê¸°
            text_model_name = MODELS['emotion_text']
            audio_model_name = MODELS['emotion_audio']
            
            print(f"   í…ìŠ¤íŠ¸ ëª¨ë¸: {text_model_name}")
            print(f"   ìŒì„± ëª¨ë¸: {audio_model_name}")
            
            # 1. í…ìŠ¤íŠ¸ ëª¨ë¸ ë¡œë”©
            self.text_tokenizer = AutoTokenizer.from_pretrained(text_model_name)
            self.text_model = AutoModelForSequenceClassification.from_pretrained(
                text_model_name
            ).to(self.device)
            
            # ========== ìˆ˜ì •: MelissaJ ëª¨ë¸ ê°•ì œ í•œê¸€ ë§¤í•‘ ==========
            if "MelissaJ" in text_model_name:
                # MelissaJ: ì˜ì–´ ë ˆì´ë¸”ì„ í•œê¸€ë¡œ ê°•ì œ ë§¤í•‘!
                self.text_labels = {
                    0: 'ê¸°ì¨',
                    1: 'ë¶„ë…¸',
                    2: 'ìƒì²˜',
                    3: 'ë¶ˆì•ˆ',
                    4: 'ë‹¹í™©',
                    5: 'ìŠ¬í””'
                }
                self.use_korean_6 = True
                print(f"   â†’ í•œêµ­ì–´ 6ê°ì • ëª¨ë¸ (ê°•ì œ í•œê¸€ ë§¤í•‘): {list(self.text_labels.values())}")
            else:
                # ê¸°ì¡´ ëª¨ë¸
                self.text_labels = self.text_model.config.id2label
                self.use_korean_6 = False
                print(f"   â†’ ê¸°ì¡´ ëª¨ë¸ (ë ˆì´ë¸”: {len(self.text_labels)}ê°œ)")
            # ======================================================

            # 2. ìŒì„± ëª¨ë¸ ë¡œë”©
            self.audio_processor = Wav2Vec2Processor.from_pretrained(audio_model_name)
            self.audio_model = Wav2Vec2ForSequenceClassification.from_pretrained(
                audio_model_name
            ).to(self.device)
            self.audio_labels = self.audio_model.config.id2label
            
            print("âœ… ê°œì„ ëœ ë©€í‹°ëª¨ë‹¬ ê°ì • ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise e

    def predict(self, audio_path, text):
        """
        ê°œì„ ëœ ê°ì • ì˜ˆì¸¡
        
        Args:
            audio_path: ìŒì„± íŒŒì¼ ê²½ë¡œ
            text: STT ê²°ê³¼ í…ìŠ¤íŠ¸
        
        Returns:
            ê°ì • ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            # === [1ë‹¨ê³„] íŠ¹ì§• ì¶”ì¶œ (Feature Extraction) ===
            
            # [Text ë¶„ì„]
            inputs = self.text_tokenizer(
                text, 
                return_tensors="pt", 
                truncation=True, 
                max_length=128
            ).to(self.device)
            
            with torch.no_grad():
                text_probs = F.softmax(self.text_model(**inputs).logits, dim=-1)
            
            text_idx = torch.argmax(text_probs).item()
            text_emotion_raw = self.text_labels[text_idx]  # â† ì´ì œ í•œê¸€!
            text_conf_raw = text_probs[0][text_idx].item()
            
            # ë””ë²„ê·¸: í…ìŠ¤íŠ¸ ëª¨ë¸ì˜ ëª¨ë“  í›„ë³´ ì¶œë ¥
            print(f"      [í…ìŠ¤íŠ¸ ê°ì • í›„ë³´]")
            for idx, prob in enumerate(text_probs[0].cpu().numpy()):
                if prob > 0.05:  # 5% ì´ìƒë§Œ
                    emotion_label = self.text_labels.get(idx, f"Unknown_{idx}")
                    print(f"         {emotion_label}: {prob:.3f}")

            # [Audio ë¶„ì„]
            y, sr = librosa.load(audio_path, sr=16000)
            target_len = 16000 * 60  # 60ì´ˆ (1ë¶„)
            if len(y) > target_len: 
                y = y[:target_len]
            else: 
                y = np.pad(y, (0, max(0, target_len - len(y))), "constant")
            
            a_inputs = self.audio_processor(
                y, 
                sampling_rate=16000, 
                return_tensors="pt", 
                padding=True
            ).input_values.to(self.device)
            
            with torch.no_grad():
                audio_probs = F.softmax(self.audio_model(a_inputs).logits, dim=-1)
                text_probs = F.softmax(self.text_model(**inputs).logits, dim=-1)
            
            audio_idx = torch.argmax(audio_probs).item()
            audio_emotion_raw = self.audio_labels[audio_idx]
            audio_conf_raw = audio_probs[0][audio_idx].item()

            # [Pitch ë¶„ì„] - Z-score ê³„ì‚°
            z_peak = self._calculate_pitch_zscore(y, sr)
            
            # === [2ë‹¨ê³„] ìƒí™©ë³„ ê°€ì¤‘ì¹˜ ì ìš© (Context-Aware Boosting) ===
            
            # í…ìŠ¤íŠ¸ ê°ì •ì€ ì´ë¯¸ í•œê¸€! (MelissaJ ê°•ì œ ë§¤í•‘)
            if self.use_korean_6:
                text_emotion_kr = text_emotion_raw  # ì´ë¯¸ í•œê¸€!
            else:
                text_emotion_kr = self._translate(text_emotion_raw)
            
            # ìŒì„± ê°ì •ì€ í•œê¸€ ë³€í™˜ í•„ìš”
            audio_emotion_kr = self._translate_audio(audio_emotion_raw)
            
            # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
            text_boost = 1.0
            audio_boost = 1.0
            boost_reason = []
            
            # â‘  í†¤ ì—­ë™ì„± ë³´ì • (Pitch Dynamics)
            if z_peak >= 2.0:
                # ê¸‰ê²©í•œ ë³€í™” â†’ ì‹¤ì œ ê²©ì–‘ë¨
                audio_boost *= 1.3
                boost_reason.append(f"í†¤ ì—­ë™ì„± ë†’ìŒ(Z={z_peak:.2f}) â†’ ìŒì„±Ã—1.3")
            elif z_peak < 1.0:
                # ë‹¨ì¡°ë¡œì›€ â†’ ì›ë˜ í†¤ì´ ë†’ê±°ë‚˜ ì˜¤ë¥˜
                audio_boost *= 0.7
                boost_reason.append(f"í†¤ ì—­ë™ì„± ë‚®ìŒ(Z={z_peak:.2f}) â†’ ìŒì„±Ã—0.7")
            
            # â‘¡ ê¸ì • ê°ì • ìˆ˜í˜¸ (Positive Override)
            if text_emotion_kr in ['ê¸°ì¨', 'í–‰ë³µ'] and text_conf_raw >= 0.8:
                # ëª…í™•í•œ ê¸ì • â†’ í…ìŠ¤íŠ¸ ìš°ì„ 
                text_boost *= 1.5
                boost_reason.append(f"ëª…í™•í•œ ê¸ì • í‘œí˜„({text_conf_raw:.2f}) â†’ í…ìŠ¤íŠ¸Ã—1.5")
            
            # â‘¢ ê°€ë©´ ìš°ìš¸ì¦ íƒì§€ (Masked Depression)
            # MelissaJëŠ” 'ì¤‘ë¦½' ì—†ìœ¼ë¯€ë¡œ ì¡°ê±´ ìˆ˜ì •
            if self.use_korean_6:
                # 6ê°ì • ëª¨ë¸: ê¸°ì¨ì´ì§€ë§Œ ìŒì„±ì€ ë¶€ì •
                if text_emotion_kr == 'ê¸°ì¨' and audio_emotion_kr in ['ìŠ¬í””', 'ë¶ˆì•ˆ']:
                    audio_boost *= 1.4
                    boost_reason.append(f"ê°€ë©´ ê°ì • ì˜ì‹¬(í…ìŠ¤íŠ¸:{text_emotion_kr}, ìŒì„±:{audio_emotion_kr}) â†’ ìŒì„±Ã—1.4")
            else:
                # ê¸°ì¡´ ëª¨ë¸: ì¤‘ë¦½ì´ì§€ë§Œ ìŒì„±ì€ ë¶€ì •
                if text_emotion_kr in ['ì¤‘ë¦½'] and audio_emotion_kr in ['ìŠ¬í””', 'ë¶ˆì•ˆ', 'ê³µí¬']:
                    audio_boost *= 1.4
                    boost_reason.append(f"ê°€ë©´ ìš°ìš¸ ì˜ì‹¬(í…ìŠ¤íŠ¸:{text_emotion_kr}, ìŒì„±:{audio_emotion_kr}) â†’ ìŒì„±Ã—1.4")
            
            # === [3ë‹¨ê³„] ìµœì¢… ì ìˆ˜ ê³„ì‚° (Scoring) ===
            
            text_score_final = text_conf_raw * text_boost
            audio_score_final = audio_conf_raw * audio_boost
            
            # Min-Max ì •ê·œí™” (0.0~1.0)
            max_score = max(text_score_final, audio_score_final)
            
            if max_score > 1.0:
                text_score_final = text_score_final / max_score
                audio_score_final = audio_score_final / max_score
            
            # === [4ë‹¨ê³„] ìµœì¢… ê²°ì • (Decision) ===
            
            score_diff = abs(text_score_final - audio_score_final)
            
            # ì•ˆì „ ê°€ì¤‘ì¹˜ (Safety Bias): ì ìˆ˜ ì°¨ì´ê°€ 0.15 ë¯¸ë§Œì´ë©´ ë¶€ì • ê°ì • ìš°ì„ 
            if score_diff < 0.15:
                # íŒë‹¨ ë¶ˆí™•ì‹¤ â†’ ë¶€ì • ê°ì • ìš°ì„  (ì•ˆì „ ì§€í–¥)
                if self.use_korean_6:
                    NEGATIVE = ['ë¶„ë…¸', 'ìŠ¬í””', 'ë¶ˆì•ˆ', 'ìƒì²˜', 'ë‹¹í™©']
                else:
                    NEGATIVE = ['ë¶„ë…¸', 'ìŠ¬í””', 'ë¶ˆì•ˆ', 'ê³µí¬', 'í˜ì˜¤']
                
                if audio_emotion_kr in NEGATIVE:
                    audio_score_final *= 1.2
                    boost_reason.append(f"íŒë‹¨ ë¶ˆí™•ì‹¤({score_diff:.3f}<0.15) â†’ ìŒì„± ë¶€ì • ìš°ì„ Ã—1.2")
                elif text_emotion_kr in NEGATIVE:
                    text_score_final *= 1.2
                    boost_reason.append(f"íŒë‹¨ ë¶ˆí™•ì‹¤({score_diff:.3f}<0.15) â†’ í…ìŠ¤íŠ¸ ë¶€ì • ìš°ì„ Ã—1.2")
            
            # ìµœì¢… ê°ì • ì„ íƒ
            if audio_score_final >= text_score_final:
                final_emotion = audio_emotion_kr
                final_conf = audio_score_final
                decision = "ìŒì„± ìš°ì„ "
            else:
                final_emotion = text_emotion_kr
                final_conf = text_score_final
                decision = "í…ìŠ¤íŠ¸ ìš°ì„ "
            
            emotion_scores = {}
        
            if self.use_korean_6:
                # MelissaJ: í•œê¸€ ë ˆì´ë¸”ë¡œ ì €ì¥
                for idx in range(len(text_probs[0])):
                    emotion_name = self.text_labels[idx]  # 'ê¸°ì¨', 'ë¶„ë…¸', ...
                    prob_value = float(text_probs[0][idx].item())
                    emotion_scores[emotion_name] = round(prob_value * 100, 2)  # í¼ì„¼íŠ¸
            else:
                # ê¸°ì¡´ ëª¨ë¸: ì˜ì–´ â†’ í•œê¸€ ë³€í™˜
                for idx in range(len(text_probs[0])):
                    emotion_name_eng = self.text_labels[idx]
                    emotion_name_kr = self._translate(emotion_name_eng)
                    prob_value = float(text_probs[0][idx].item())
                    emotion_scores[emotion_name_kr] = round(prob_value * 100, 2)
        
            print(f"      [ê°ì • ì ìˆ˜] {emotion_scores}")
            
            return {
                # ì›ë³¸ ê²°ê³¼
                'text_emotion': text_emotion_kr,  # ì´ì œ í•œê¸€!
                'text_conf': text_conf_raw,
                'audio_emotion': audio_emotion_kr, 
                'audio_conf': audio_conf_raw,
                
                # ê°œì„ ëœ ê²°ê³¼
                'text_score_boosted': float(text_score_final),
                'audio_score_boosted': float(audio_score_final),
                'z_peak': float(z_peak),
                'boost_reason': boost_reason,
                'decision': decision,
                
                'candidates': emotion_scores,
                
                # ìµœì¢… ê²°ê³¼
                'final_emotion': final_emotion,
                'final_conf': float(final_conf)
            }
            
        except Exception as e:
            print(f"âš ï¸ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {
                'final_emotion': 'ì•Œìˆ˜ì—†ìŒ',
                'audio_emotion': 'ì•Œìˆ˜ì—†ìŒ',
                'text_emotion': 'ì•Œìˆ˜ì—†ìŒ',
                'text_conf': 0.5,
                'audio_conf': 0.5,
                'final_conf': 0.5,
                'z_peak': 0.0,
                'boost_reason': [],
                'decision': 'ì˜¤ë¥˜'
            }

    def _calculate_pitch_zscore(self, y, sr, sigma_min=5.0):
        """
        Pitch Z-score ê³„ì‚°
        
        Args:
            y: ì˜¤ë””ì˜¤ ì‹ í˜¸
            sr: ìƒ˜í”Œë§ ë ˆì´íŠ¸
            sigma_min: ìµœì†Œ í‘œì¤€í¸ì°¨ ì„ê³„ê°’ (ê¸°ë³¸ 5.0 Hz)
        
        Returns:
            z_peak: ìµœëŒ€ Z-score ì ˆëŒ“ê°’
        
        ìˆ˜ì‹:
            Z_peak = max(|F0(t) - Î¼_F0| / max(Ïƒ_F0, Ïƒ_min))
        """
        try:
            # F0 ì¶”ì¶œ (Fundamental Frequency)
            f0, voiced_flag, voiced_probs = librosa.pyin(
                y, 
                fmin=librosa.note_to_hz('C2'),  # ìµœì†Œ 65.4 Hz
                fmax=librosa.note_to_hz('C7'),  # ìµœëŒ€ 2093 Hz
                sr=sr
            )
            
            # NaN ì œê±° (ë¬´ì„±ìŒ êµ¬ê°„)
            f0_valid = f0[~np.isnan(f0)]
            
            if len(f0_valid) < 10:
                # ìœ íš¨í•œ í”¼ì¹˜ê°€ ë„ˆë¬´ ì ìœ¼ë©´ 0 ë°˜í™˜
                return 0.0
            
            # í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ê³„ì‚°
            mu_f0 = np.mean(f0_valid)
            sigma_f0 = np.std(f0_valid)
            
            # ì•ˆì „ ìƒìˆ˜ ì ìš© (í‘œì¤€í¸ì°¨ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ sigma_min ì‚¬ìš©)
            sigma_safe = max(sigma_f0, sigma_min)
            
            # Z-score ê³„ì‚°
            z_scores = np.abs((f0_valid - mu_f0) / sigma_safe)
            
            # ìµœëŒ“ê°’ ë°˜í™˜ (ìˆœê°„ì ì¸ ê²©ì–‘ í¬ì°©)
            z_peak = np.max(z_scores)
            
            return z_peak
            
        except Exception as e:
            print(f"âš ï¸ Pitch Z-score ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0.0

    def _translate_audio(self, label):
        """
        ìŒì„± ê°ì • ë ˆì´ë¸”ë§Œ í•œê¸€ ë³€í™˜
        (í…ìŠ¤íŠ¸ ê°ì •ì€ ì´ë¯¸ í•œê¸€ì´ë¯€ë¡œ ë³€í™˜ ì•ˆ í•¨!)
        """
        label = str(label).lower()
        
        # ìŒì„± ëª¨ë¸ ë ˆì´ë¸” ë§¤í•‘ë§Œ
        audio_mapping = {
            'angry': 'ë¶„ë…¸',
            'fear': 'ë¶ˆì•ˆ',
            'happy': 'ê¸°ì¨',
            'neutral': 'ì¤‘ë¦½',
            'sad': 'ìŠ¬í””',
            # ìˆ«ì ë ˆì´ë¸”
            '0': 'ë¶„ë…¸',
            '1': 'ê¸°ì¨',
            '2': 'ë¶ˆì•ˆ',
            '3': 'ìŠ¬í””',
            '4': 'ì¤‘ë¦½',
        }
        
        for k, v in audio_mapping.items():
            if k in label:
                return v
        
        return 'ì¤‘ë¦½'

    def _translate(self, label):
        """
        ê¸°ì¡´ ëª¨ë¸ìš© ê°ì • ë ˆì´ë¸” í•œê¸€ ë³€í™˜
        (MelissaJëŠ” ì´ë¯¸ í•œê¸€ì´ë¼ í˜¸ì¶œ ì•ˆ ë¨)
        """
        label = str(label).lower()
        mapping = {
            'anger': 'ë¶„ë…¸', 'angry': 'ë¶„ë…¸',
            'disgust': 'í˜ì˜¤', 'disgusted': 'í˜ì˜¤',
            'fear': 'ê³µí¬', 'fearful': 'ê³µí¬',
            'happiness': 'ê¸°ì¨', 'happy': 'ê¸°ì¨',
            'neutral': 'ì¤‘ë¦½',
            'sadness': 'ìŠ¬í””', 'sad': 'ìŠ¬í””',
            'surprise': 'ë†€ëŒ', 'surprised': 'ë†€ëŒ',
            'embarrassed': 'ë‹¹í™©',
            'heartache': 'ìŠ¬í””',
            '0': 'ê³µí¬', '1': 'ë†€ëŒ', '2': 'ë¶„ë…¸', '3': 'ìŠ¬í””', 
            '4': 'ì¤‘ë¦½', '5': 'ê¸°ì¨', '6': 'í˜ì˜¤'
        }
        
        for k, v in mapping.items():
            if k in label: 
                return v
        return 'ì¤‘ë¦½'