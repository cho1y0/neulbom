# llm_handler.py
import os
import threading
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple

from dotenv import load_dotenv


class LLMHandler:
    """
    ë³´ë¯¸ LLM í•¸ë“¤ëŸ¬ (ì¡´ëŒ“ë§ ê³ ì • + ì„¸ì…˜ë³„ íˆìŠ¤í† ë¦¬ ë¶„ë¦¬ + SDK í˜¸í™˜)

    âœ… í•´ê²° í¬ì¸íŠ¸
    - openai Python SDK (êµ¬ë²„ì „/ì‹ ë²„ì „) ëª¨ë‘ ì§€ì›
    - ëª¨ë¸ ì ‘ê·¼ ë¶ˆê°€/ëª¨ë¸ëª… ì˜¤ë¥˜ ì‹œ fallback ëª¨ë¸ë¡œ ìë™ ì¬ì‹œë„
    - ì„¸ì…˜ë³„ history ë¶„ë¦¬ (ì„ì„ ë°©ì§€)
    - ì¡´ëŒ“ë§ ê³ ì • + ë°˜ë§ ê°ì§€ ì‹œ 1íšŒ ì¬ì‘ì„±
    """

    def __init__(
        self,
        model: Optional[str] = None,
        temperature: float = 0.3,
        max_completion_tokens: int = 900,
        max_turns: int = 10,
    ):
        print("â³ OpenAI ì´ˆê¸°í™” ì¤‘...")

        # 1) .env ë¡œë”© (ê¸°ì¡´ êµ¬ì¡° ìœ ì§€)
        env_path = Path(__file__).parent / "api-key" / "openapi.env"
        if env_path.exists():
            load_dotenv(dotenv_path=env_path)
            print(f"   ğŸ”‘ í‚¤ íŒŒì¼ ë¡œë”©: {env_path}")
        else:
            print(f"   âš ï¸ í‚¤ íŒŒì¼ì„ ëª» ì°¾ì•˜ìŠµë‹ˆë‹¤: {env_path}")

        # 2) API í‚¤ í™•ì¸
        self.api_key = os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤. api-key/openapi.envë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”.")

        # 3) ëª¨ë¸ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ ìš°ì„ )
        env_model = os.getenv("OPENAI_MODEL", "").strip()
        # ê¸°ë³¸ê°’ì€ ì ‘ê·¼ì„±ì´ ë†’ì€ ëª¨ë¸ë¡œ (gpt-5-miniëŠ” ê³„ì •ì— ë”°ë¼ ë¯¸ì§€ì›ì¼ ìˆ˜ ìˆìŒ)
        self.primary_model = (model or env_model or "gpt-4o-mini").strip()

        # fallback ëª¨ë¸ ëª©ë¡ (í•„ìš” ì‹œ ì¶”ê°€ ê°€ëŠ¥)
        # - primary_model ë¨¼ì € ì‹œë„ â†’ ì‹¤íŒ¨í•˜ë©´ ì•„ë˜ ìˆœì„œëŒ€ë¡œ ì‹œë„
        self.model_fallbacks = self._build_model_fallbacks(self.primary_model)

        self.temperature = float(temperature)
        self.max_completion_tokens = int(max_completion_tokens)
        self.max_turns = int(max_turns)

        # 4) SDK í˜¸í™˜ ì´ˆê¸°í™” (ì‹ ë²„ì „ ìš°ì„ , ì‹¤íŒ¨ ì‹œ êµ¬ë²„ì „)
        self._sdk_mode, self._client = self._init_openai_client(self.api_key)

        # 5) ì„¸ì…˜ë³„ íˆìŠ¤í† ë¦¬ + ë½
        self._histories: Dict[str, List[Dict[str, str]]] = {}
        self._lock = threading.Lock()

        print(f"âœ… OpenAI ì¤€ë¹„ ì™„ë£Œ (mode={self._sdk_mode}, model={self.primary_model})")

    # -------------------------
    # Public
    # -------------------------
    def chat(
        self,
        user_input: str,
        emotion_info: Optional[Dict[str, Any]] = None,
        scores: Optional[Dict[str, Any]] = None,
        max_turns: Optional[int] = None,
        session_id: Optional[str] = None,
        extra_context: Optional[str] = None,
    ) -> str:
        if not user_input or not user_input.strip():
            return "ë„¤, ì–´ë¥´ì‹ . ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"

        sid = (session_id or "default").strip() or "default"
        turns_limit = int(max_turns) if max_turns is not None else self.max_turns

        system_prompt = self._build_system_prompt(emotion_info, scores, extra_context)

        with self._lock:
            history = self._histories.get(sid)
            if not history:
                history = [{"role": "system", "content": system_prompt}]
                self._histories[sid] = history
            else:
                history[0] = {"role": "system", "content": system_prompt}

            history.append({"role": "user", "content": user_input.strip()})
            self._trim_history_locked(history, turns_limit)
            messages = list(history)  # ìŠ¤ëƒ…ìƒ·

        # LLM í˜¸ì¶œ (ëª¨ë¸ fallback í¬í•¨)
        ai_response, used_model = self._call_with_fallback(messages)

        ai_response = (ai_response or "").strip()
        ai_response = self._ensure_polite(ai_response)

        # ë°˜ë§/í†¤ í”ë“¤ë¦¼ ê°ì§€ ì‹œ 1íšŒ ì¬ì‘ì„±
        if self._looks_like_banmal(ai_response):
            ai_response = self._rewrite_to_polite(
                draft=ai_response,
                user_input=user_input,
                system_prompt=system_prompt,
                model=used_model,
            )
            ai_response = self._ensure_polite(ai_response)

        with self._lock:
            history = self._histories.get(sid, [{"role": "system", "content": system_prompt}])
            history.append({"role": "assistant", "content": ai_response})
            self._trim_history_locked(history, turns_limit)
            self._histories[sid] = history

        return ai_response or "ì£„ì†¡í•©ë‹ˆë‹¤, ì–´ë¥´ì‹ . ë‹¤ì‹œ í•œ ë²ˆ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?"

    def reset_conversation(self, session_id: Optional[str] = None) -> None:
        with self._lock:
            if session_id is None:
                self._histories.clear()
                print("ğŸ§¹ ì „ì²´ ëŒ€í™” ì´ˆê¸°í™”")
            else:
                sid = (session_id or "default").strip() or "default"
                self._histories.pop(sid, None)
                print(f"ğŸ§¹ ì„¸ì…˜ ëŒ€í™” ì´ˆê¸°í™”: {sid}")

    # -------------------------
    # OpenAI init (SDK í˜¸í™˜)
    # -------------------------
    def _init_openai_client(self, api_key: str) -> Tuple[str, Any]:
        # 1) ì‹ ë²„ì „ SDK ì‹œë„: from openai import OpenAI
        try:
            from openai import OpenAI  # type: ignore
            client = OpenAI(api_key=api_key)
            return "v1", client
        except Exception:
            pass

        # 2) êµ¬ë²„ì „ SDK: import openai; openai.api_key=...
        try:
            import openai  # type: ignore
            openai.api_key = api_key
            return "legacy", openai
        except Exception as e:
            raise RuntimeError(f"openai SDK ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    def _build_model_fallbacks(self, primary: str) -> List[str]:
        # ì¤‘ë³µ ì œê±° + ì•ˆì •ì ì¸ í›„ë³´ë“¤
        candidates = [primary, "gpt-4o-mini", "gpt-4.1-mini", "gpt-4o"]
        out = []
        for m in candidates:
            if m and m not in out:
                out.append(m)
        return out

    # -------------------------
    # Call with fallback models
    # -------------------------
    def _call_with_fallback(self, messages: List[Dict[str, str]]) -> Tuple[str, str]:
        last_err = None
        for model in self.model_fallbacks:
            try:
                text = self._call_chat_completion(model, messages)
                return text, model
            except Exception as e:
                last_err = e
                msg = str(e)
                print(f"âŒ OpenAI í˜¸ì¶œ ì‹¤íŒ¨ (model={model}): {type(e).__name__}: {msg}")

                # ëª¨ë¸ ë¯¸ì§€ì›/ëª¨ë¸ëª… ì˜¤ë¥˜ì¼ ë•ŒëŠ” ë‹¤ìŒ ëª¨ë¸ë¡œ ê³„ì†
                lowered = msg.lower()
                if "model" in lowered and ("not found" in lowered or "does not exist" in lowered or "no such model" in lowered):
                    continue
                # ê·¸ ì™¸(í‚¤/ë„¤íŠ¸ì›Œí¬/ê¶Œí•œ/ì¿¼í„° ë“±)ëŠ” ê³„ì† ì‹œë„í•´ë„ ì˜ë¯¸ ì—†ì„ ìˆ˜ ìˆì–´ ì¤‘ë‹¨
                break

        # ì—¬ê¸°ê¹Œì§€ ì™”ìœ¼ë©´ ì „ë¶€ ì‹¤íŒ¨
        raise RuntimeError(f"OpenAI í˜¸ì¶œ ì‹¤íŒ¨: {last_err}")

    def _call_chat_completion(self, model: str, messages: List[Dict[str, str]]) -> str:
        if self._sdk_mode == "v1":
            # ì‹ ë²„ì „
            resp = self._client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=self.temperature,
                max_completion_tokens=self.max_completion_tokens,
            )
            return (resp.choices[0].message.content or "").strip()

        # êµ¬ë²„ì „
        # (êµ¬ë²„ì „ì€ íŒŒë¼ë¯¸í„°ê°€ max_tokensì¸ ê²½ìš°ê°€ ë§ìŒ)
        resp = self._client.ChatCompletion.create(
            model=model,
            messages=messages,
            temperature=self.temperature,
            max_tokens=self.max_completion_tokens,
        )
        return (resp["choices"][0]["message"]["content"] or "").strip()

    # -------------------------
    # Prompts
    # -------------------------
    def _build_system_prompt(
        self,
        emotion_info: Optional[Dict[str, Any]],
        scores: Optional[Dict[str, Any]],
        extra_context: Optional[str],
    ) -> str:
        parts = [self._build_base_prompt()]

        if extra_context:
            parts.append("ã€ì¶”ê°€ ë§¥ë½ã€‘\n" + str(extra_context).strip())

        if emotion_info:
            parts.append(self._build_emotion_prompt(emotion_info))

        if scores:
            rp = self._build_risk_prompt(scores)
            if rp:
                parts.append(rp)

        return "\n\n".join(parts).strip()

    def _build_base_prompt(self) -> str:
        return (
            "ë‹¹ì‹ ì€ ì–´ë¥´ì‹ ì„ ë„ì™€ë“œë¦¬ëŠ” ëŒë´„ ëŒ€í™” ë„ìš°ë¯¸ 'ë³´ë¯¸'ì…ë‹ˆë‹¤.\n\n"
            "ã€í•„ìˆ˜ ê·œì¹™ã€‘\n"
            "- ë°˜ë“œì‹œ ì¡´ëŒ“ë§(í•˜ì‹­ì‹œì˜¤/í•˜ì„¸ìš”ì²´)ë¡œë§Œ ë‹µë³€í•©ë‹ˆë‹¤. ë°˜ë§/ì¹œêµ¬ë§íˆ¬/ë¹„ì†ì–´ëŠ” ê¸ˆì§€í•©ë‹ˆë‹¤.\n"
            "- AI/ëª¨ë¸/í”„ë¡¬í”„íŠ¸ ê°™ì€ ê¸°ìˆ  ì„¤ëª…ì€ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n"
            "- í•œ ë²ˆì— 1~2ë¬¸ì¥ìœ¼ë¡œ ì§§ê³  ë˜ë ·í•˜ê²Œ ë‹µí•©ë‹ˆë‹¤.\n"
            "- ë¨¼ì € ê³µê° 1ë¬¸ì¥ â†’ í•µì‹¬ ì•ˆë‚´ 1ë¬¸ì¥ ìˆœì„œë¡œ ë‹µí•©ë‹ˆë‹¤.\n"
            "- ì˜ë£Œ/ì§„ë‹¨ì€ í•˜ì§€ ë§ê³ , í•„ìš”í•˜ë©´ ì˜ë£Œì§„ ìƒë‹´ì„ ê¶Œí•©ë‹ˆë‹¤.\n"
        )

    def _build_emotion_prompt(self, emotion_info: Dict[str, Any]) -> str:
        final_emotion = emotion_info.get("final_emotion", "ì¤‘ë¦½")
        conf = float(emotion_info.get("audio_conf", 0.5) or 0.5)

        prompt = (
            "ã€í˜„ì¬ ê°ì • ìƒíƒœã€‘\n"
            f"- ê°ì •: {final_emotion}\n"
            f"- í™•ì‹ ë„: {conf:.2f}\n\n"
            "ã€ëŒ€í™” ì „ëµã€‘\n"
        )

        if final_emotion == "ìŠ¬í””":
            prompt += "- ë”°ëœ»í•˜ê²Œ ê³µê°í•˜ê³  ìœ„ë¡œí•©ë‹ˆë‹¤. ë¶€ë‹´ ì—†ëŠ” ì§ˆë¬¸ 1ê°œë§Œ ë§ë¶™ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        elif final_emotion == "ë¶„ë…¸":
            prompt += "- ì°¨ë¶„íˆ ê²½ì²­í•˜ê³  ê°ì •ì„ ì¸ì •í•©ë‹ˆë‹¤. ë…¼ìŸí•˜ê±°ë‚˜ ì§€ì í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
        elif final_emotion == "ë¶ˆì•ˆ":
            prompt += "- ì•ˆì‹¬ì‹œí‚¤ê³  ì§€ê¸ˆ í•  ìˆ˜ ìˆëŠ” ê°„ë‹¨í•œ í–‰ë™ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì„ íƒì§€ëŠ” ìµœëŒ€ 2ê°œì…ë‹ˆë‹¤."
        elif final_emotion == "ê³µí¬":
            prompt += "- ë§¤ìš° ë¶€ë“œëŸ½ê³  ì•ˆì •ì ìœ¼ë¡œ ë§í•©ë‹ˆë‹¤. ì•ˆì „ì„ í™•ì¸í•˜ëŠ” ì§§ì€ ì•ˆë‚´ë¥¼ ì œê³µí•©ë‹ˆë‹¤."
        elif final_emotion == "ê¸°ì¨":
            prompt += "- í•¨ê»˜ ê¸°ë»í•˜ê³  ê¸ì •ì ìœ¼ë¡œ ë§ì¥êµ¬ì¹©ë‹ˆë‹¤. ì§ˆë¬¸ì€ 1ê°œë§Œ í•©ë‹ˆë‹¤."
        else:
            prompt += "- í¸ì•ˆí•œ í†¤ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”ë¥¼ ì´ì–´ê°‘ë‹ˆë‹¤."

        return prompt

    def _build_risk_prompt(self, scores: Dict[str, Any]) -> Optional[str]:
        avg = float(scores.get("average", 100) or 100)
        emo = float(scores.get("emotion", 100) or 100)

        if avg < 50 or emo < 40:
            return (
                "ã€ì£¼ì˜: ê³ ìœ„í—˜ ìƒíƒœ ê°€ëŠ¥ã€‘\n"
                "- ë” ë¶€ë“œëŸ½ê²Œ í™•ì¸ ì§ˆë¬¸ì„ 1ê°œ í¬í•¨í•©ë‹ˆë‹¤.\n"
                "- ìœ„ê¸‰ ì§•í›„ê°€ ì˜ì‹¬ë˜ë©´ ë³´í˜¸ì ë˜ëŠ” ì˜ë£Œì§„ ë„ì›€ì„ ê¶Œí•©ë‹ˆë‹¤."
            )
        if avg < 65 or emo < 60:
            return (
                "ã€ì£¼ì˜: ê´€ì‹¬ í•„ìš”ã€‘\n"
                "- ìœ„ë¡œì™€ ê²©ë ¤ë¥¼ í¬í•¨í•˜ê³  ë¬´ë¦¬í•œ ì§ˆë¬¸ì€ í”¼í•©ë‹ˆë‹¤."
            )
        return None

    # -------------------------
    # History
    # -------------------------
    def _trim_history_locked(self, history: List[Dict[str, str]], max_turns: int) -> None:
        keep = (max_turns * 2) + 1  # system 1 + (user/assistant)*max_turns
        if len(history) > keep:
            history[:] = [history[0]] + history[-(keep - 1):]

    # -------------------------
    # Politeness guardrails
    # -------------------------
    def _ensure_polite(self, text: str) -> str:
        if not text:
            return "ë„¤, ì–´ë¥´ì‹ . ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
        t = text.strip()
        # ë„ˆë¬´ ê¸¸ë©´ ì¤„ë°”ê¿ˆ ê³¼ë‹¤ë§Œ ì»·
        if t.count("\n") >= 8:
            t = "\n".join(t.splitlines()[:8]).strip()
        return t

    def _looks_like_banmal(self, text: str) -> bool:
        if not text:
            return False
        markers = ["ì•¼", "í•´ë¼", "í–ˆëƒ", "í•˜ì§€ë§ˆ", "ì•Œê² ì–´", "ëì–´", "ë­ì•¼", "í•´ë´", "í•˜ì", "í–ˆì–´"]
        return any(m in text for m in markers)

    def _rewrite_to_polite(self, draft: str, user_input: str, system_prompt: str, model: str) -> str:
        prompt = (
            "ì•„ë˜ ì´ˆì•ˆì„ ì–´ë¥´ì‹ ê»˜ ë“œë¦¬ëŠ” ë‹µë³€ìœ¼ë¡œ ë‹¤ì‹œ ì‘ì„±í•˜ì„¸ìš”.\n"
            "- ë°˜ë“œì‹œ ì¡´ëŒ“ë§(í•˜ì‹­ì‹œì˜¤/í•˜ì„¸ìš”ì²´)\n"
            "- 1~2ë¬¸ì¥\n"
            "- ê³µê° 1ë¬¸ì¥ + í•µì‹¬ ì•ˆë‚´ 1ë¬¸ì¥\n\n"
            f"ã€ì‚¬ìš©ì ë§ì”€ã€‘ {user_input}\n"
            f"ã€ì´ˆì•ˆã€‘ {draft}\n"
        )
        try:
            if self._sdk_mode == "v1":
                resp = self._client.chat.completions.create(
                    model=model,
                    messages=[{"role": "system", "content": system_prompt},
                              {"role": "user", "content": prompt}],
                    temperature=max(0.1, min(self.temperature, 0.4)),
                    max_completion_tokens=min(350, self.max_completion_tokens),
                )
                out = (resp.choices[0].message.content or "").strip()
                return out or "ì£„ì†¡í•©ë‹ˆë‹¤, ì–´ë¥´ì‹ . ë‹¤ì‹œ í•œ ë²ˆ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?"
            else:
                resp = self._client.ChatCompletion.create(
                    model=model,
                    messages=[{"role": "system", "content": system_prompt},
                              {"role": "user", "content": prompt}],
                    temperature=max(0.1, min(self.temperature, 0.4)),
                    max_tokens=min(350, self.max_completion_tokens),
                )
                out = (resp["choices"][0]["message"]["content"] or "").strip()
                return out or "ì£„ì†¡í•©ë‹ˆë‹¤, ì–´ë¥´ì‹ . ë‹¤ì‹œ í•œ ë²ˆ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?"
        except Exception as e:
            print(f"âš ï¸ ì¡´ëŒ“ë§ ì¬ì‘ì„± ì‹¤íŒ¨: {type(e).__name__}: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤, ì–´ë¥´ì‹ . ë‹¤ì‹œ í•œ ë²ˆ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?"
