"""
LLM í”„ë¡¬í”„íŠ¸ A/B/C í…ŒìŠ¤íŠ¸
3ê°€ì§€ ë²„ì „ ë¹„êµ
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline
from config.models import MODELS


class LLMHandler:
    """3ê°€ì§€ í”„ë¡¬í”„íŠ¸ ë²„ì „ í…ŒìŠ¤íŠ¸"""
    
    def __init__(self, prompt_version="B", use_cpu_offload=True):
        """
        Args:
            prompt_version: "A", "B", "C"
                A: ì˜ˆì‹œ ì—†ìŒ (ìµœì†Œ)
                B: ì˜ˆì‹œ 2ê°œ (ì¤‘ê°„)
                C: ì˜ˆì‹œ 5ê°œ (ìµœëŒ€)
        """
        print(f"â³ LLM ë¡œë”© ì¤‘ (ë²„ì „ {prompt_version})...")
        self.prompt_version = prompt_version
        self.use_cpu_offload = use_cpu_offload
        self.load_model()
        print("âœ… LLM ë¡œë”© ì™„ë£Œ!")
        
        self.history = []
    
    def load_model(self):
        """ëª¨ë¸ ë¡œë“œ"""
        model_name = MODELS['llm']
        options = MODELS['llm_options']
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        if torch.cuda.is_available():
            print(f"   ğŸ’¾ GPU VRAM: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f}GB")
        
        quantization_config = None
        if options['load_in_4bit'] and torch.cuda.is_available():
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                llm_int8_enable_fp32_cpu_offload=self.use_cpu_offload
            )
        
        device_map = "auto"
        max_memory = {0: "7GB", "cpu": "30GB"} if self.use_cpu_offload else None
        
        try:
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                quantization_config=quantization_config,
                device_map=device_map,
                max_memory=max_memory,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                low_cpu_mem_usage=True,
            )
            print(f"   âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        except Exception as e:
            print(f"   âš ï¸  GPU ì‹¤íŒ¨, CPU ì „í™˜")
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name, device_map="cpu", torch_dtype=torch.float32
            )
        
        self.pipe = pipeline(
            "text-generation",
            model=self.model,
            tokenizer=self.tokenizer,
            max_new_tokens=200,
            temperature=0.7,
            top_p=0.9,
            repetition_penalty=1.15,
            do_sample=True,
        )
    
    def get_system_prompt(self):
        """ë²„ì „ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸"""
        
        if self.prompt_version == "A":
            # ========== ë²„ì „ A: ì˜ˆì‹œ ì—†ìŒ (ìµœì†Œ) ==========
            return """ë„ˆëŠ” í• ë¨¸ë‹ˆì˜ ì†ë…€ 'ë³´ë¯¸'ì•¼.

ê·œì¹™:
- AI í‹° ë‚´ì§€ ë§ˆ
- ì œ3ìì²˜ëŸ¼ ë§í•˜ì§€ ë§ê³  ì§ì ‘ ëŒ€í™”í•´
- ì§§ê²Œ 1-2ë¬¸ì¥

ì§€ê¸ˆ í• ë¨¸ë‹ˆì™€ ëŒ€í™” ì¤‘ì´ì•¼."""
        
        elif self.prompt_version == "B":
            # ========== ë²„ì „ B: ì˜ˆì‹œ 2ê°œ (ì¤‘ê°„) ==========
            return """ë„ˆëŠ” í• ë¨¸ë‹ˆì˜ ì†ë…€ 'ë³´ë¯¸'ì•¼.

ê·œì¹™:
- AI í‹° ë‚´ì§€ ë§ˆ
- ì œ3ìì²˜ëŸ¼ ë§í•˜ì§€ ë§ê³  ì§ì ‘ ëŒ€í™”í•´
- ì§§ê²Œ 1-2ë¬¸ì¥

"ë„Œ ëˆ„êµ¬ì•¼?" â†’ "í• ë¨¸ë‹ˆ, ì € ë³´ë¯¸ì–ì•„ìš”!"
"ë°°ê³ íŒŒ" â†’ "ë­ ë“œì‹œê³  ì‹¶ìœ¼ì„¸ìš”?"

ì§€ê¸ˆ í• ë¨¸ë‹ˆì™€ ëŒ€í™” ì¤‘ì´ì•¼."""
        
        else:  # version == "C"
            # ========== ë²„ì „ C: ì˜ˆì‹œ 5ê°œ (ìµœëŒ€) ==========
            return """ë„ˆëŠ” í• ë¨¸ë‹ˆì˜ ì†ë…€ 'ë³´ë¯¸'ì•¼.

ê·œì¹™:
- AI í‹° ë‚´ì§€ ë§ˆ
- ì œ3ìì²˜ëŸ¼ ë§í•˜ì§€ ë§ê³  ì§ì ‘ ëŒ€í™”í•´
- ì§§ê²Œ 1-2ë¬¸ì¥

ì˜ˆì‹œ:
"ë„Œ ëˆ„êµ¬ì•¼?" â†’ "í• ë¨¸ë‹ˆ, ì € ë³´ë¯¸ì–ì•„ìš”! ì†ë…€!"
"ë°°ê³ íŒŒ" â†’ "ì•„ì´ê³ , ë°°ê³ í”„ì‹œêµ¬ë‚˜! ë­ ë“œì‹œê³  ì‹¶ìœ¼ì„¸ìš”?"
"ì‹¬ì‹¬í•´" â†’ "ì‹¬ì‹¬í•˜ì‹œêµ¬ë‚˜~ ì œê°€ ì´ì•¼ê¸° ë“¤ì–´ë“œë¦´ê²Œìš”!"
"ë¨¸ë¦¬ ì•„íŒŒ" â†’ "ì—êµ¬, ë¨¸ë¦¬ ì•„í”„ì„¸ìš”? ì•½ ë“œì…¨ì–´ìš”?"
"ì˜¤ëŠ˜ ë‚ ì”¨ ì¢‹ë„¤" â†’ "ë„¤! ì •ë§ ì¢‹ì€ ë‚ ì”¨ì˜ˆìš”~ ì‚°ì±… ê°€ì‹¤ë˜ìš”?"

ì§€ê¸ˆ í• ë¨¸ë‹ˆì™€ ëŒ€í™” ì¤‘ì´ì•¼."""
    
    def chat(self, user_input, max_turns=10):
        """ëŒ€í™”"""
        if not self.history:
            self.history.append({
                "role": "system", 
                "content": self.get_system_prompt()
            })
        
        self.history.append({"role": "user", "content": user_input})
        response = self._run_pipeline(self.history)
        self.history.append({"role": "assistant", "content": response})
        
        if len(self.history) > max_turns * 2 + 1:
            self.history = [self.history[0]] + self.history[-(max_turns * 2):]
        
        return response
    
    def reset_conversation(self):
        """ì´ˆê¸°í™”"""
        self.history = []
    
    def _run_pipeline(self, messages):
        """ìƒì„±"""
        try:
            prompt = self.tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            outputs = self.pipe(
                prompt,
                eos_token_id=self.tokenizer.eos_token_id,
                pad_token_id=self.tokenizer.pad_token_id
            )
            generated_text = outputs[0]['generated_text']
            return generated_text[len(prompt):].strip()
        except Exception as e:
            return "í• ë¨¸ë‹ˆ, ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?"


# ========== A/B/C í…ŒìŠ¤íŠ¸ ==========
def compare_versions():
    """3ê°œ ë²„ì „ ë¹„êµ í…ŒìŠ¤íŠ¸"""
    
    test_cases = [
        "ë„Œ ëˆ„êµ¬ì•¼?",
        "ë°°ê³ íŒŒ",
        "ì‹¬ì‹¬í•´",
        "ë¨¸ë¦¬ ì•„íŒŒ",
        "ì˜¤ëŠ˜ ë‚ ì”¨ ì¢‹ë„¤",
        "ì˜ ëª¨ë¥´ê² ì–´",  # ì˜ˆì‹œì— ì—†ëŠ” ì¼€ì´ìŠ¤
    ]
    
    print("="*70)
    print("ğŸ§ª A/B/C í…ŒìŠ¤íŠ¸: í”„ë¡¬í”„íŠ¸ ë²„ì „ ë¹„êµ")
    print("="*70)
    print("\në²„ì „:")
    print("  A: ì˜ˆì‹œ ì—†ìŒ (ìµœì†Œ)")
    print("  B: ì˜ˆì‹œ 2ê°œ (ì¤‘ê°„)")
    print("  C: ì˜ˆì‹œ 5ê°œ (ìµœëŒ€)")
    print("\n" + "="*70)
    
    results = {}
    
    for version in ["A", "B", "C"]:
        print(f"\n{'='*70}")
        print(f"ë²„ì „ {version} í…ŒìŠ¤íŠ¸ ì¤‘...")
        print("="*70)
        
        llm = LLMHandler(prompt_version=version)
        results[version] = []
        
        for i, question in enumerate(test_cases, 1):
            print(f"\n[{i}/{len(test_cases)}] í• ë¨¸ë‹ˆ: {question}")
            response = llm.chat(question)
            print(f"ë³´ë¯¸: {response}")
            
            results[version].append({
                "question": question,
                "response": response
            })
            
            llm.reset_conversation()
        
        print()
    
    # ê²°ê³¼ ë¹„êµ
    print("\n" + "="*70)
    print("ğŸ“Š ê²°ê³¼ ë¹„êµ")
    print("="*70)
    
    for i, question in enumerate(test_cases):
        print(f"\nì§ˆë¬¸ {i+1}: \"{question}\"")
        print("-" * 70)
        for version in ["A", "B", "C"]:
            response = results[version][i]["response"]
            print(f"  ë²„ì „ {version}: {response}")
    
    print("\n" + "="*70)
    print("ğŸ’¡ í‰ê°€ ê¸°ì¤€")
    print("="*70)
    print("1. ìì—°ìŠ¤ëŸ¬ìš´ê°€?")
    print("2. ì†ë…€ ì—­í• ì„ ìœ ì§€í•˜ëŠ”ê°€?")
    print("3. ì œ3ì í™”ë²•ì„ ì“°ëŠ”ê°€? (ë‚˜ì¨)")
    print("4. ì˜ˆì‹œì— ì—†ëŠ” ì§ˆë¬¸ë„ ì˜ ë‹µí•˜ëŠ”ê°€?")
    print("5. ì°½ì˜ì ì´ê³  ë‹¤ì–‘í•œê°€?")
    print("\nì–´ëŠ ë²„ì „ì´ ê°€ì¥ ì¢‹ì€ì§€ ì§ì ‘ íŒë‹¨í•´ë³´ì„¸ìš”!")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "compare":
        # ë¹„êµ ëª¨ë“œ
        compare_versions()
    else:
        # ë‹¨ì¼ í…ŒìŠ¤íŠ¸ (ê¸°ë³¸: ë²„ì „ B)
        version = sys.argv[1] if len(sys.argv) > 1 else "B"
        
        print(f"ë²„ì „ {version} ë‹¨ì¼ í…ŒìŠ¤íŠ¸")
        llm = LLMHandler(prompt_version=version)
        
        test_cases = ["ë„Œ ëˆ„êµ¬ì•¼?", "ë°°ê³ íŒŒ", "ì‹¬ì‹¬í•´"]
        
        for msg in test_cases:
            print(f"\ní• ë¨¸ë‹ˆ: {msg}")
            response = llm.chat(msg)
            print(f"ë³´ë¯¸: {response}")
            llm.reset_conversation()
